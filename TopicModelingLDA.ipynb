{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport re\nimport string\n\nimport spacy\n\nimport gensim\nfrom gensim import corpora\n\nreview_data= pd.read_csv(\"Topic_Modeling.csv\")","metadata":{},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def clean_text(text ): \n    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n    delete_dict[' '] = ' ' \n    table = str.maketrans(delete_dict)\n    text1 = text.translate(table)\n    #print('cleaned:'+text1)\n    textArr= text1.split()\n    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>3))]) \n    \n    return text2.lower()","metadata":{},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords') # run this one time","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"review_data.dropna(axis = 0, how ='any',inplace=True)\n\nreview_data['Text'] = review_data['Text'].apply(clean_text)\nreview_data['Num_words_text'] = review_data['Text'].apply(lambda x:len(str(x).split())) \n\nprint('-------Dataset --------')\nprint(review_data['Score'].value_counts())\nprint(len(review_data))\nprint('-------------------------')\nmax_review_data_sentence_length  = review_data['Num_words_text'].max()\n\nmask = (review_data['Num_words_text'] < 100) & (review_data['Num_words_text'] >=20)\ndf_short_reviews = review_data[mask]\ndf_sampled = df_short_reviews.groupby('Score').apply(lambda x: x.sample(n=20000)).reset_index(drop = True)\n\nprint('No of Short tweets')\nprint(len(df_short_reviews))\n\n\n\n#all_sentences = train_data['text'].tolist() + test_data['text'].tolist()\n","metadata":{},"execution_count":53,"outputs":[{"name":"stdout","output_type":"stream","text":"-------Dataset --------\n5    363111\n4     80655\n1     52264\n3     42638\n2     29743\nName: Score, dtype: int64\n568411\n-------------------------\nNo of Short reviews\n373281\n"}]},{"cell_type":"markdown","source":"***\nLet us pre-process the data\n***","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n# function to remove stopwords\ndef remove_stopwords(text):\n    textArr = text.split(' ')\n    rem_text = \" \".join([i for i in textArr if i not in stop_words])\n    return rem_text\n\n# remove stopwords from the text\ndf_sampled['Text']=df_sampled['Text'].apply(remove_stopwords)\n\n","metadata":{},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n\ndef lemmatization(texts,allowed_postags=['NOUN', 'ADJ']): \n       output = []\n       for sent in texts:\n             doc = nlp(sent) \n             output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])\n       return output","metadata":{},"execution_count":55,"outputs":[]},{"cell_type":"code","source":" \ntext_list=df_sampled['Text'].tolist()\nprint(text_list[1])\ntokenized_reviews = lemmatization(text_list)\nprint(tokenized_reviews[1])","metadata":{},"execution_count":56,"outputs":[{"name":"stdout","output_type":"stream","text":"love stroopwafels tried sorts kind come shipper stroopie stroopwafels nice brand instead count advertised arrived fancy blue need even sticker says count either someone embezzling stroopwafels misleadingly advertised aware read count makes price point pretty different\n['love', 'stroopwafel', 'sort', 'shipper', 'stroopie', 'nice', 'brand', 'fancy', 'blue', 'need', 'sticker', 'stroopwafel', 'aware', 'read', 'count', 'price', 'point', 'different']\n"}]},{"cell_type":"markdown","source":"***\nCreate vocabulary dictionary and document term matrix\n***","metadata":{}},{"cell_type":"code","source":"dictionary = corpora.Dictionary(tokenized_reviews)\ndoc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_reviews]","metadata":{},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Creating the object for LDA model using gensim library\nLDA = gensim.models.ldamodel.LdaModel\n\n# Build LDA model\nlda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=10, random_state=100,\n                chunksize=1000, passes=50,iterations=100)","metadata":{},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"lda_model.print_topics()","metadata":{},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"[(0,\n  '0.036*\"flavor\" + 0.033*\"chip\" + 0.028*\"snack\" + 0.028*\"good\" + 0.026*\"great\" + 0.024*\"salt\" + 0.017*\"nice\" + 0.016*\"candy\" + 0.015*\"perfect\" + 0.014*\"little\"'),\n (1,\n  '0.078*\"product\" + 0.053*\"free\" + 0.025*\"gluten\" + 0.018*\"yummy\" + 0.017*\"cracker\" + 0.017*\"snack\" + 0.011*\"package\" + 0.010*\"pretzel\" + 0.010*\"shipping\" + 0.010*\"banana\"'),\n (2,\n  '0.047*\"sauce\" + 0.035*\"cereal\" + 0.032*\"good\" + 0.027*\"cheese\" + 0.021*\"pasta\" + 0.017*\"fresh\" + 0.017*\"seed\" + 0.015*\"beef\" + 0.014*\"texture\" + 0.013*\"taste\"'),\n (3,\n  '0.041*\"product\" + 0.039*\"time\" + 0.031*\"order\" + 0.028*\"great\" + 0.018*\"good\" + 0.016*\"small\" + 0.014*\"package\" + 0.013*\"size\" + 0.013*\"bag\" + 0.013*\"soup\"'),\n (4,\n  '0.024*\"bread\" + 0.022*\"time\" + 0.018*\"product\" + 0.018*\"year\" + 0.014*\"ginger\" + 0.013*\"clean\" + 0.013*\"pill\" + 0.012*\"stomach\" + 0.011*\"mild\" + 0.011*\"week\"'),\n (5,\n  '0.091*\"food\" + 0.030*\"treat\" + 0.024*\"dog\" + 0.020*\"good\" + 0.014*\"great\" + 0.014*\"cat\" + 0.014*\"healthy\" + 0.013*\"meal\" + 0.013*\"little\" + 0.012*\"ingredient\"'),\n (6,\n  '0.090*\"price\" + 0.085*\"store\" + 0.038*\"good\" + 0.037*\"local\" + 0.034*\"great\" + 0.029*\"grocery\" + 0.017*\"cheap\" + 0.016*\"shipping\" + 0.015*\"well\" + 0.014*\"product\"'),\n (7,\n  '0.040*\"flavor\" + 0.032*\"taste\" + 0.027*\"water\" + 0.026*\"good\" + 0.023*\"sugar\" + 0.019*\"drink\" + 0.016*\"sweet\" + 0.015*\"fruit\" + 0.014*\"great\" + 0.013*\"product\"'),\n (8,\n  '0.056*\"chocolate\" + 0.035*\"good\" + 0.034*\"cookie\" + 0.025*\"milk\" + 0.024*\"butter\" + 0.023*\"great\" + 0.021*\"peanut\" + 0.021*\"taste\" + 0.019*\"bar\" + 0.017*\"flavor\"'),\n (9,\n  '0.143*\"coffee\" + 0.032*\"flavor\" + 0.030*\"good\" + 0.026*\"strong\" + 0.017*\"taste\" + 0.016*\"bean\" + 0.015*\"kcup\" + 0.014*\"blend\" + 0.013*\"great\" + 0.013*\"morning\"')]"},"metadata":{}}]},{"cell_type":"code","source":"model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=doc_term_matrix, texts=tokenized_reviews, start=2, limit=50, step=1)","metadata":{},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Select the model and print the topics\noptimal_model = model_list[7]\nmodel_topics = optimal_model.show_topics(formatted=False)\noptimal_model.print_topics(num_words=10)","metadata":{},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"[(0,\n  '0.080*\"chocolate\" + 0.050*\"cookie\" + 0.034*\"butter\" + 0.031*\"peanut\" + 0.020*\"milk\" + 0.019*\"taste\" + 0.018*\"great\" + 0.016*\"flavor\" + 0.016*\"good\" + 0.014*\"oatmeal\"'),\n (1,\n  '0.036*\"chip\" + 0.025*\"salt\" + 0.025*\"good\" + 0.024*\"flavor\" + 0.018*\"great\" + 0.014*\"product\" + 0.013*\"store\" + 0.011*\"love\" + 0.011*\"fresh\" + 0.010*\"time\"'),\n (2,\n  '0.039*\"treat\" + 0.025*\"dog\" + 0.021*\"time\" + 0.020*\"small\" + 0.018*\"product\" + 0.015*\"great\" + 0.014*\"size\" + 0.013*\"little\" + 0.011*\"year\" + 0.011*\"large\"'),\n (3,\n  '0.129*\"coffee\" + 0.047*\"flavor\" + 0.031*\"good\" + 0.027*\"strong\" + 0.021*\"taste\" + 0.015*\"vanilla\" + 0.013*\"bean\" + 0.013*\"blend\" + 0.013*\"great\" + 0.012*\"green\"'),\n (4,\n  '0.048*\"water\" + 0.027*\"drink\" + 0.021*\"taste\" + 0.020*\"bottle\" + 0.019*\"bread\" + 0.017*\"flavor\" + 0.015*\"energy\" + 0.012*\"cold\" + 0.012*\"product\" + 0.012*\"good\"'),\n (5,\n  '0.036*\"good\" + 0.031*\"sweet\" + 0.031*\"sugar\" + 0.028*\"flavor\" + 0.024*\"snack\" + 0.023*\"taste\" + 0.018*\"great\" + 0.017*\"fruit\" + 0.017*\"calorie\" + 0.016*\"healthy\"'),\n (6,\n  '0.026*\"good\" + 0.023*\"sauce\" + 0.017*\"great\" + 0.013*\"flavor\" + 0.013*\"popcorn\" + 0.012*\"taste\" + 0.011*\"soup\" + 0.011*\"product\" + 0.010*\"pasta\" + 0.010*\"time\"'),\n (7,\n  '0.111*\"food\" + 0.020*\"good\" + 0.017*\"cat\" + 0.015*\"brand\" + 0.015*\"ingredient\" + 0.012*\"baby\" + 0.012*\"healthy\" + 0.011*\"product\" + 0.010*\"organic\" + 0.009*\"stuff\"'),\n (8,\n  '0.044*\"price\" + 0.038*\"product\" + 0.035*\"store\" + 0.031*\"great\" + 0.030*\"good\" + 0.015*\"order\" + 0.015*\"local\" + 0.014*\"time\" + 0.012*\"shipping\" + 0.011*\"grocery\"')]"},"metadata":{}}]}]}